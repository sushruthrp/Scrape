{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "page_url = 'http://boards.4chan.org/hr/thread/3393622'\n",
    "\n",
    "req = Request(url = page_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "page_webcode = urlopen(req).read()\n",
    "soup = BeautifulSoup(page_webcode, 'html.parser')\n",
    "fuzzy = (soup.findAll('a', {'class':'fileThumb','href': True}))#   (soup.findAll('a', {'class':'fileThumb','href': True}))\n",
    "#print((fuzzy))\n",
    "links = [fuzz.get('href') for fuzz in fuzzy]\n",
    "#print(links)\n",
    "#print((fuzzy[1].get('href')))\n",
    "\n",
    "name_ = soup.find('span', {'class':'name'})\n",
    "#print((name_))\n",
    "\n",
    "number_ = soup.find('span', {'class':'postNum desktop' })\n",
    "#print(number_)\n",
    "\n",
    "name_try = soup.findAll('div',{'class':'fileText'})\n",
    "names_list = [(name_try[i].find('a',href=True)) for i in range(0,len(links))]\n",
    "#print(name_try[0])\n",
    "#name_again = name_try.find('',{'a href':str(links[0])})\n",
    "#print(name_again)\n",
    "#for a in name_try.findAll('a', href=True):\n",
    "#    print (\"Found the URL:\", a['href'])\n",
    "#print(name_try.find('a',href=True))\n",
    "print(len(links))\n",
    "print(len([(name_try[i].find('a',href=True)) for i in range(0,len(links))]))\n",
    "\n",
    "def getFileName(i):\n",
    "    if (names_list[i].get('title')!= None):\n",
    "        return names_list[i].get('title')\n",
    "    else:\n",
    "        name = str(names_list[i]).split('_blank\">')[1].split('</a>')[0]\n",
    "        return name\n",
    "#    name = str(names_list[i]).split('_blank\">')[1].split('</a>')[0]\n",
    "#    return name\n",
    "#    file_link = links[i]\n",
    "#    name_try = soup.find('div',{'class':'fileText'})\n",
    "#    name = str(name_try)\n",
    "\n",
    "#print(names_list[6])\n",
    "#print(names_list[5])\n",
    "#print([name.text for name in names_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to find 1\n"
     ]
    }
   ],
   "source": [
    "#page_url.split('boards.')[1].split('.org')[0]\n",
    "a = 1\n",
    "print('to find', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hr\n",
      "spaceman\n",
      "3393622\n",
      "Success\n",
      "0\n",
      "1\n",
      "2\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home = 'E:/'\n",
    "chan = '4chan'\n",
    "board = page_url.split('org/')[1].split('/thread')[0]\n",
    "print(board)\n",
    "name = str(name_).split('\"name\">')[1].split('</span>')[0]\n",
    "print(name)\n",
    "\n",
    "number = str(number_).split('#p')[1].split('\"')[0]\n",
    "print(number)\n",
    "\n",
    "path_ = home+chan+'/'+board+'/'+name+'-'+number+'/'\n",
    "try:\n",
    "    os.makedirs(path_)\n",
    "except OSError:\n",
    "    print('Creation in %s failed' %home)\n",
    "else:\n",
    "    print('Success')\n",
    "    \n",
    "    \n",
    "    \n",
    "#import urllib.request\n",
    "\n",
    "#opener = urllib.request.build_opener()\n",
    "#opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "#urllib.request.install_opener(opener)\n",
    "#urllib.request.urlretrieve(\"type URL here\", \"path/file_name\")\n",
    "    \n",
    "    \n",
    "    \n",
    "j = 0\n",
    "while(j<3):\n",
    "    filename = os.path.join(path_, getFileName(j))\n",
    "    urllib.request.urlretrieve('https:'+links[j], filename)\n",
    "    print(j)\n",
    "    j+=1\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//i.4cdn.org/wsg/1573410312450.webm\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "page_url = 'http://boards.4channel.org/wsg/thread/3138861'\n",
    "\n",
    "req = Request(url = page_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "page_webcode = urlopen(req).read()\n",
    "soup = BeautifulSoup(page_webcode, 'html.parser')\n",
    "\n",
    "video_links = soup.findAll('a', {'class':'fileThumb','href': True})\n",
    "#print((video_links[0]))\n",
    "l1 = video_links[0].get('href')\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
